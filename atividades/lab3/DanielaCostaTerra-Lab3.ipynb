{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"c4HO0","launcher_item_id":"lSYZM"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"DanielaCostaTerra-Lab3.ipynb","provenance":[{"file_id":"19t3uX49x-5g7cnCcK0yYx7ZJp05lrSQH","timestamp":1617656646201}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Sl-F1Cp0cZuY"},"source":["# Lab 3 - BCC406\n","\n","## REDES NEURAIS E APRENDIZAGEM EM PROFUNDIDADE\n","\n","## Construindo uma rede neural\n","\n","### Prof. Eduardo e Prof. Pedro Silva\n","\n","##Aluna: Daniela Costa Terra\n","\n","Data da entrega : 15/04 \n","\n","- Complete o código (marcado com ToDo) e quando requisitado, escreva textos diretamente nos notebooks. Onde tiver *None*, substitua pelo seu código.\n","- Execute todo notebook e salve tudo em um PDF **nomeado** como \"NomeSobrenome-Lab3.pdf\"\n","- Envie o PDF para pelo [FORM](https://forms.gle/LdGDeFYH6wQm9ahh6)"]},{"cell_type":"markdown","metadata":{"id":"P1TpbxeTLfkU"},"source":["# **Parte 1** - Rede neural do zero: passo a passo (10pt)\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ytBld9uz9a-f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409603762,"user_tz":180,"elapsed":4674,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"e22e7a99-d592-42ec-b173-e71e43a23d1a"},"execution_count":138,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**Notação**:\n","- Sobrescrito índice $ [l] $ indica os valores associados a $l$-ésima camada.\n","    - **Exemplo:** $ a^{[l]} $ é a ativação da $l$-ésima camada.\n","- Sobrescrito índice $ (i) $ indica os valores associados ao $i$-ésima exemplo.\n","    - **Exemplo:** $ x^{(i)} $ é o  $i$-ésima exemplo de treinamento.\n","- Subescrito índice $ j $ indica a $j$-ésima entrada de um vetor.\n","    - **Exemplo:** $a^{[l]}_j$ indica a $j$-ésima  entrada  da ativação da $ l$-ésima camada."],"metadata":{"id":"H_ltW_dGP5MU"}},{"cell_type":"markdown","source":["## 1 - Importação dos pacotes\n"],"metadata":{"id":"rTcwU5csP-Px"}},{"cell_type":"markdown","metadata":{"id":"QAOVC8TLLfkV"},"source":["Primeiro, vamos executar a célula abaixo para importar todos os pacotes que precisaremos.\n","- [numpy](www.numpy.org) é o pacote fundamental para a computação científica com Python.\n","- [h5py](http://www.h5py.org) é um pacote comum para interagir com um conjunto de dados armazenado em um arquivo H5.\n","- [matplotlib](http://matplotlib.org) é uma biblioteca famosa para plotar gráficos em Python.\n","- [PIL](http://www.pythonware.com/products/pil/) e [scipy](https://www.scipy.org/) são usados aqui para testar seu modelo.\n","- dnn_utils fornece algumas funções necessárias para este notebook.\n","- testCases fornece alguns casos de teste para avaliar as funções.\n","- np.random.seed (1) é usado para manter todas as chamadas de funções aleatórias. "]},{"cell_type":"code","metadata":{"id":"J9BlvNrOLl3M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409608248,"user_tz":180,"elapsed":4495,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"a1734836-2bb9-4623-f905-a80a7d0863a8"},"source":["# Para Google Colab: Você vai precisar fazer o upload dos arquivos no seu drive e montá-lo\n","# não se esqueça de ajustar o path para o seu drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"CEWo1hJ4TpN9","executionInfo":{"status":"ok","timestamp":1650409608250,"user_tz":180,"elapsed":70,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# Você vai precisar inserir seu diretório para importar as \"bibliotecas próprias\" auxiliares deste notebook\n","# não se esqueça de ajustar o path para o seu diretório\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/disciplinasDoutorado/PCC177-2022-1(Redes)/lab3')"],"execution_count":140,"outputs":[]},{"cell_type":"code","metadata":{"id":"ne48u9hjLfkX","executionInfo":{"status":"ok","timestamp":1650409608251,"user_tz":180,"elapsed":68,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"729bde4e-ab07-448f-891b-7bd4bab5996e"},"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","# bibliotecas auxiliares (ver testCases_v4a.py e dnn_utils_v2.py)\n","from testCases_v4a import *\n","from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n","#\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"execution_count":141,"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}]},{"cell_type":"markdown","metadata":{"id":"8jF5eYx4Lfkc"},"source":["## 2 - Esboço das Funções auxiliares\n"]},{"cell_type":"markdown","source":["\n","\n","- Inicialização dos parâmetros da rede.\n","- Implementação da fase forward propagation (roxo na figura abaixo).\n","     - Complete a parte LINEAR da etapa de forward propagation de uma camada (resultando em $ Z^{[l]}$).\n","     - Fornecemos a função ATIVAÇÃO (relu / sigmóide).\n","     - Combine os dois passos anteriores em uma nova função de avanço [LINEAR-> ATIVAÇÃO].\n","     - Empilhe a função de avanço [LINEAR-> RELU] L-1 (para as camadas 1 a L-1) e adicione um [LINEAR-> SIGMOID] no final (para a camada final $ L $). Isso fornece uma nova função L_model_forward.\n","- Cálculo a função loss.\n","- Implementação da fase backward propagation (vermelho na figura abaixo).\n","    - Complete a parte LINEAR da etapa de backward propagation de uma camada.\n","    - Fornecemos o gradiente da função (relu_backward / sigmoid_backward)\n","    - Combine as duas etapas anteriores em uma nova função [LINEAR-> ATIVAÇÃO] para trás.\n","    - Empilhe [LINEAR-> RELU] para trás L-1 vezes e adicione [LINEAR-> SIGMOID] para trás em uma nova função L_model_backward\n","- Atualização dos parâmetros.\n","\n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=19iowxbfZWLXFvB6eGP0hcRZcsQFltmHk)\n","\n","<caption><center> **Figura 1**</center></caption><br>\n","\n","**Observe** que para todas as etapas forward, existe uma etapa backward correspondente. É por isso que em cada etapa forward você estará armazenando alguns valores em cache. Os valores em cache são úteis para calcular gradientes. Na etapa backward, você usará o cache para calcular os gradientes. "],"metadata":{"id":"oXNJeQVVPyA1"}},{"cell_type":"markdown","metadata":{"id":"sSEMNMlOLfkd"},"source":["## 3 - Inicialização (1pt)\n"]},{"cell_type":"markdown","source":["\n","A função será usada para inicializar parâmetros para uma rede com $ L $-camadas.\n","\n","### 3.1 - Rede Neural com $L$-camadas\n","\n","**Instruções**:\n","- A estrutura do modelo é * [LINEAR -> RELU] $ \\times $ (L-1) -> LINEAR -> SIGMOID *. Ou seja, possui (L-1) camadas  usando uma função de ativação ReLU seguida por uma camada de saída com uma função de ativação sigmóide.\n","- Use inicialização aleatória para as matrizes de peso. Use `np.random.randn(shape) * 0,01`.\n","- Use a inicialização de zeros para os vieses. Use `np.zeros(shape)`.\n","- Armazenaremos $ n ^ {[l]} $, o número de elementos/neurônios na camada $l$, em uma variável `camadas_dims`. Por exemplo, `camadas_dims = [2,4,1]` é uma rede com duas entradas, uma camada oculta com 4 unidades/neurônios e uma camada de saída com 1 unidade/neurônio de saída . "],"metadata":{"id":"Zm-gglS4Psce"}},{"cell_type":"code","metadata":{"id":"HL745AoeLfke","executionInfo":{"status":"ok","timestamp":1650409608251,"user_tz":180,"elapsed":63,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# Inicialize_parametros\n","\n","def inicialize_parametros(camadas_dims):\n","    \"\"\"\n","    Entrada:\n","    camadas_dims -- python array (lista) contendo a dimensão de cada camada da rede\n","    \n","    \n","    Saída:\n","    parametros   -- python dicionario contendo os parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- vetor de pesos com formato (camadas_dims[l], camadas_dims[l-1])\n","                    bl -- vetor de vies com formato (camadas_dims[l], 1)\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parametros = {}\n","    L = len(camadas_dims)         # ToDo: número de camadas da rede\n","\n","    ### Início do código ### \n","    for l in range(1, L):\n","      # dica: itere pelo número de camadas, inicializando pesos e viés de cada camada,\n","      # e armazenem em parameters (≈ 2 linhas de código)\n","      parametros['W' + str(l)] = np.random.randn(camadas_dims[l], camadas_dims[l-1]) * 0.01 # ToDo   \n","      parametros['b' + str(l)] = np.zeros((camadas_dims[l], 1)) # ToDo    \n","    ### Fim do código ###\n","        \n","    return parametros"],"execution_count":142,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1HPweLSNLfkh"},"source":["**Comentários** \n","Ao concluir o `inicialize_parametros`, certifique-se de que as dimensões entre cada camada estejam corretas. Lembre-se de que $ n ^ {[l]} $ é o número de unidades na camada $ l $. Assim, por exemplo, se o tamanho da nossa entrada $ X $ for $ (12288, 209) $ (com número de exemplos $ m = 209 $), então:\n","\n","<table style=\"width:100%\">\n","\n","\n","   <tr>\n","        <td>  </td> \n","        <td> **Formato de W** </td> \n","        <td> **Formato de b**  </td> \n","        <td> **Ativação** </td>\n","         <td> **Formato da Ativação** </td> \n","    <tr>\n","    \n","   <tr>\n","        <td> **Camada 1** </td> \n","        <td> $(n^{[1]},12288)$ </td> \n","        <td> $(n^{[1]},1)$ </td> \n","        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td>         \n","        <td> $(n^{[1]},209)$ </td> \n","    <tr>\n","    \n","   <tr>\n","        <td> **Camada 2** </td> \n","        <td> $(n^{[2]}, n^{[1]})$  </td> \n","        <td> $(n^{[2]},1)$ </td> \n","        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n","        <td> $(n^{[2]}, 209)$ </td> \n","    <tr>\n","   \n","   <tr>\n","        <td> $\\vdots$ </td> \n","        <td> $\\vdots$  </td> \n","        <td> $\\vdots$  </td> \n","        <td> $\\vdots$</td> \n","        <td> $\\vdots$  </td> \n","    <tr>\n","    \n","   <tr>\n","        <td> **Camada L-1** </td> \n","        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n","        <td> $(n^{[L-1]}, 1)$  </td> \n","        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n","        <td> $(n^{[L-1]}, 209)$ </td> \n","    <tr>\n","    \n","    \n","   <tr>\n","        <td> **Camada L** </td> \n","        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n","        <td> $(n^{[L]}, 1)$ </td>\n","        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n","        <td> $(n^{[L]}, 209)$  </td> \n","    <tr>\n","\n","</table>\n"]},{"cell_type":"code","metadata":{"id":"e9iMuR52Lfkh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409608252,"user_tz":180,"elapsed":62,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"7f228432-0e5e-446f-9fce-2cd75af489b6"},"source":["# Teste\n","parametros = inicialize_parametros([5,4,3])\n","print(\"W1 = \" + str(parametros[\"W1\"]))\n","print(\"b1 = \" + str(parametros[\"b1\"]))\n","print(\"W2 = \" + str(parametros[\"W2\"]))\n","print(\"b2 = \" + str(parametros[\"b2\"]))"],"execution_count":143,"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n"," [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n"," [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n"," [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n"," [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n","b2 = [[0.]\n"," [0.]\n"," [0.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"bPW7fW79Lfkl"},"source":["**Valores esperados**:\n","       \n","<table style=\"width:80%\">\n","  <tr>\n","    <td> **W1** </td>\n","    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n"," [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n"," [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n","  </tr>\n","  \n","  <tr>\n","    <td>**b1** </td>\n","    <td>[[ 0.]\n"," [ 0.]\n"," [ 0.]\n"," [ 0.]]</td> \n","  </tr>\n","  \n","  <tr>\n","    <td>**W2** </td>\n","    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n"," [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n"," [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n","  </tr>\n","  \n","  <tr>\n","    <td>**b2** </td>\n","    <td>[[ 0.]\n"," [ 0.]\n"," [ 0.]]</td> \n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Mbb4pJFaLfkl"},"source":["## 4 - Fase: Forward propagation (2pt)\n"]},{"cell_type":"markdown","source":["\n","Usaremos duas funções:\n","- LINEAR\n","- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID\n","\n","### 4.1 - Linear Forward \n","\n","A função linear_forward (sobre todos os examples) é definida pela equação:\n","\n","$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n","\n","onde $A^{[0]} = X$. \n","\n","\n","**Lembrete**\n","Lembre-se de que quando calculamos $ W X + b $ em python, ele realiza `broadcasting`. Por exemplo, se:\n","$$ W = \\begin{bmatrix}\n","    j  & k  & l\\\\\n","    m  & n & o \\\\\n","    p  & q & r \n","\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n","    a  & b  & c\\\\\n","    d  & e & f \\\\\n","    g  & h & i \n","\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n","    s  \\\\\n","    t  \\\\\n","    u\n","\\end{bmatrix}\\tag{2}$$\n","\n","Então $WX + b$ será:\n","\n","$$ WX + b = \\begin{bmatrix}\n","    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n","    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n","    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n","\\end{bmatrix}\\tag{3}  $$"],"metadata":{"id":"WzsLpjbHPhjY"}},{"cell_type":"code","metadata":{"id":"p2zuS4HdLfkn","executionInfo":{"status":"ok","timestamp":1650409608253,"user_tz":180,"elapsed":51,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# Função linear_forward\n","\n","def linear_forward(A, W, b):\n","    \"\"\"\n","    Implementa a parte linear da fase de propogação nas camadas\n","\n","    Entradas:\n","    A - dados de entrada da camada atual (ativações da camada anterior): formato (tamanho da camada anterior, número de exemplos)\n","    W - matriz de pesos: matriz numpy com formato (tamanho da camada atual, tamanho da camada anterior)\n","    b - vetor de viés, matriz numpy com formato (tamanho da camada atual, 1)\n","\n","    Saídas:\n","    Z -- a entrada da função de ativação, também chamada de parâmetro de pré-ativação\n","    cache - uma tupla python contendo \"A\", \"W\" e \"b\"; (armazenado para usar na fase backward propagation)\n","    \"\"\"\n","    \n","    ### Início do código ### (≈ 2 linhas de código)\n","    Z = W.dot(A) + b # dica: use a funçao .dot() \n","    cache = (A, W, b)\n","    ### Fim do código ###\n","    \n","    return Z, cache"],"execution_count":144,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yaYF_aMLfkr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409608254,"user_tz":180,"elapsed":51,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"8c2b4ec0-27f5-4429-b5e0-0be09cd3a256"},"source":["# Teste\n","A, W, b = linear_forward_test_case()\n","\n","Z, linear_cache = linear_forward(A, W, b)\n","Z, linear_cache"],"execution_count":145,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 3.26295337, -1.23429987]]), (array([[ 1.62434536, -0.61175641],\n","         [-0.52817175, -1.07296862],\n","         [ 0.86540763, -2.3015387 ]]),\n","  array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]),\n","  array([[-0.24937038]])))"]},"metadata":{},"execution_count":145}]},{"cell_type":"markdown","metadata":{"id":"y0zOgOn1Lfku"},"source":["**Valores Esperados**:\n","\n","<table style=\"width:35%\">\n","  \n","  <tr>\n","    <td> **Z** </td>\n","    <td> [[ 3.26295337 -1.23429987]] </td> \n","  </tr>\n","  \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"4ky5XExaLfkv"},"source":["### 4.2 - Linear-Ativação Forward\n","\n","Usaremos duas funções de ativação:\n","\n","- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. A função `sigmoid`, **retorna dois** itens: o valor de ativação \"` a` \"e um\" `cache`\" que contém \"` Z` \"(necessário para a fase backward correspondente). Para usá-lo, basta chamar: \n","``` python\n","A, ativacao_cache = sigmoid(Z)\n","```\n","\n","- **ReLU**: A formula é $A = RELU(Z) = max(0, Z)$. A função `relu`, **retorna dois** itens: o valor de ativação \"` a` \"e um\" `cache`\" que contém \"` Z` \"(necessário para a fase backward correspondente). Para usá-lo, basta chamar:\n","``` python\n","A, ativacao_cache = relu(Z)\n","```"]},{"cell_type":"markdown","metadata":{"id":"_Yoy96_gLfkv"},"source":["**Exercício**: Implemente a *LINEAR-> ATIVAÇÃO* da camada da fase forward propagation. A relação matemática é: $ A^{[l]} = g (Z^{[l]}) = g (W^{[l]} A^{[l-1]} + b^{[l]} ) $ onde a ativação \"g\" pode ser `sigmoid` ou `relu`. Use a função linear_forward ()."]},{"cell_type":"code","metadata":{"id":"5ia-RStILfkw","executionInfo":{"status":"ok","timestamp":1650409608255,"user_tz":180,"elapsed":47,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# Função linear_ativacao_forward\n","\n","def linear_ativacao_forward(A_prev, W, b, ativacao):\n","    \"\"\"\n","    Implementa a *LINEAR-> ATIVAÇÃO* da camada da fase forward propagation\n","\n","    Entradas:\n","    A_prev -- dados de entrada da camada atual (ativações da camada anterior): formato (tamanho da camada anterior, número de exemplos)\n","    W - matriz de pesos: matriz numpy com formato (tamanho da camada atual, tamanho da camada anterior)\n","    b - vetor de viés, matriz numpy com formato (tamanho da camada atual, 1)\n","    ativacao -- \"sigmoid\" ou \"relu\"\n","\n","    Saídas:\n","    A -- a saída da função de ativação, também chamada de valor da pós-ativação\n","    cache -- uma tupla python contendo \"linear_cache\" e \"ativacao_cache\"; \n","    (armazenado para usar na fase backward propagation)          \n","    \"\"\"\n","    A = np.array([])\n","    cache = ()\n","    if ativacao == \"sigmoid\":\n","        # Entradas: \"A_prev, W, b\". Saídas: \"A, ativacao_cache\".\n","        ### Início do código ###\n","        # dicas: use sua funcao de propagação e as funções de ativação fornecidas em dnn_utils_v2.py\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z) \n","        cache = (linear_cache, activation_cache)\n","        ### Fim do código ###\n","    \n","    elif ativacao == \"relu\":\n","        # Entradas: \"A_prev, W, b\". Saídas: \"A, ativacao_cache\".\n","        ### Início do código ### \n","        # dicas: use sua funcao de propagação e as funções de ativação fornecidas em dnn_utils_v2.py\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","        cache = (linear_cache, activation_cache)\n","        ### Fim do código ###\n","    \n","    return A, cache"],"execution_count":146,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_AkFIuvLfk0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409608256,"user_tz":180,"elapsed":47,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"cdb20cb7-8978-4b58-9a71-8dfb88bde175"},"source":["# Teste\n","\n","A_prev, W, b = linear_activation_forward_test_case()\n","\n","A, linear_ativacao_cache = linear_ativacao_forward(A_prev, W, b, ativacao = \"sigmoid\")\n","print(\"com sigmoid: A = \" + str(A))\n","\n","A, linear_ativacao_cache = linear_ativacao_forward(A_prev, W, b, ativacao = \"relu\")\n","print(\"com ReLU: A = \" + str(A))\n","\n"],"execution_count":147,"outputs":[{"output_type":"stream","name":"stdout","text":["com sigmoid: A = [[0.96890023 0.11013289]]\n","com ReLU: A = [[3.43896131 0.        ]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"uzUZyUGZLfk3"},"source":["**Valores esperados**:\n","       \n","<table style=\"width:35%\">\n","  <tr>\n","    <td> **com sigmoid: A ** </td>\n","    <td > [[ 0.96890023  0.11013289]]</td> \n","  </tr>\n","  <tr>\n","    <td> **com ReLU: A ** </td>\n","    <td > [[ 3.43896131  0.        ]]</td> \n","  </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"v0xQWVgsLfk4"},"source":["### d) Modelo de L-camadas\n","\n","Replica a função `linear_ativacao_forward` com RELU $(L-1)$ vezes, depois uma vez `linear_ativacao_forward` com SIGMOID.\n","\n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=19bhG0GkUjw3fLBTmoKjDZImWcq2nE2GD)\n","\n","\n","<caption><center> **Figura 2** : Esquema do modelo *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* </center></caption><br>\n","\n","**Instrução**: A variável `AL` é $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (ativaçnao da última camada, i.e., $\\hat{Y}$.) "]},{"cell_type":"code","metadata":{"id":"Iz4OPSekLfk4","executionInfo":{"status":"ok","timestamp":1650409608257,"user_tz":180,"elapsed":44,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# L_modelo_forward\n","\n","def L_modelo_forward(X, parametros):\n","    \"\"\"\n","    Implementa a fase forward propagation \n","    \n","    Entradas:\n","    X -- dados, numpy array de tamanho (input size, number of examples)\n","    parametros -- parametros iniciais\n","    \n","    Saídas:\n","    AL -- valor da pós-ativação da última camada\n","    caches -- lista dos caches contendo:\n","                todos caches da linear_ativacao_forward() (existem L-1 deles, indexados de 1 a L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X                      # dados da camada inicial\n","    L = int(len(parametros)/2)      # números de camadas da rede\n","    \n","    # Implemente [LINEAR -> RELU]*(L-1). Adicione o \"linear_cache\" para a lista \"caches\".\n","    ### Início do código ### \n","    for l in range(1, L):\n","        A_prev = A         \n","        A, cache = linear_ativacao_forward(A_prev, parametros[\"W\"+str(l)], parametros[\"b\"+str(l)], ativacao = \"relu\")    # dica: utilize linear_activation_forward()\n","        caches.append(cache)\n","    ### Fim do código ###\n","    \n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","    ### Início do código ### \n","    AL, cache =   linear_ativacao_forward(A, parametros[\"W\"+str(L)], parametros[\"b\"+str(L)], ativacao = \"sigmoid\") # dica : utilize linear_activation_forward()\n","    caches.append(cache)\n","    ### Fim do código ###\n","    \n","               \n","    return AL, caches"],"execution_count":148,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mK30kYjLfk7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409608258,"user_tz":180,"elapsed":43,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"4329c690-8a0f-4e5d-8971-e9c8ee28fa08"},"source":["X, parametros = L_model_forward_test_case_2hidden()\n","AL, caches = L_modelo_forward(X, parametros)\n","\n","print(\"AL = \" + str(AL))\n","print(\"Tamanho da lista caches = \" + str(len(caches)))"],"execution_count":149,"outputs":[{"output_type":"stream","name":"stdout","text":["AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n","Tamanho da lista caches = 3\n"]}]},{"cell_type":"markdown","metadata":{"id":"9CG1VCWELfk_"},"source":["**Valores Esperados:**\n","\n","<table style=\"width:50%\">\n","  <tr>\n","    <td> **AL** </td>\n","    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n","  </tr>\n","  <tr>\n","    <td> **Tamanho da lista caches ** </td>\n","    <td > 3 </td> \n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Qil7E18JLfk_"},"source":["Usando $A^{[L]}$, você deve calcular o custo da rede."]},{"cell_type":"markdown","metadata":{"id":"SiqOiRqvLflC"},"source":["## 5 - Função Custo (cross-entropy) (2pt)\n","\n"]},{"cell_type":"markdown","source":["\n","Para a fase backward propagation é necessário o cálculo da funcão custo.\n","\n","**Exercício**: Use a seguinte função custo: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\left(y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)\\right) \\tag{7}$$\n","\n","obs.: veja que é a mesma implementada para o Lab1b."],"metadata":{"id":"rBGpvFlFParB"}},{"cell_type":"code","metadata":{"id":"2UDSELo2LflD","executionInfo":{"status":"ok","timestamp":1650409608259,"user_tz":180,"elapsed":40,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# Função custo\n","\n","def custo(AL, Y):\n","    \"\"\"\n","    Implementa a função custo da rede.\n","\n","    Entradas:\n","    AL -- Probabiliade de predição da rede, (1, numero de exemplos)\n","    Y -- Vetor de rótulos dos exemplos de treinamento  ( 0 se não tem gato, 1 tem gato ), (1, numero de exemplos)\n","\n","    Saída:\n","    custo -- custo da rede\n","    \"\"\"\n","    \n","    m = Y.shape[1] # número de exemplos\n","\n","    # Compute loss from AL and y.\n","    ###Início do código ### (≈ 1 linha de código)\n","    custo = -1/m*np.sum(np.sum(Y*np.log(AL) + (1 - Y)*np.log(1 - AL), axis=0))\n","    ### Fim do código ###\n","    \n","    custo = np.squeeze(custo)      # assegurar o formato experado ( [[17]] para 17).\n","    \n","    \n","    return custo"],"execution_count":150,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhS4QgwnLflH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409608260,"user_tz":180,"elapsed":39,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"7d1c3d2c-a2f4-4947-d9fb-ffb2841a90f7"},"source":["Y, AL = compute_cost_test_case()\n","\n","print(\"custo = \" + str(custo(AL, Y)))"],"execution_count":151,"outputs":[{"output_type":"stream","name":"stdout","text":["custo = 0.2797765635793422\n"]}]},{"cell_type":"markdown","metadata":{"id":"g_ew5cpfLflL"},"source":["**Valores Esperados**:\n","\n","<table>\n","\n","   <tr>\n","    <td>**custo** </td>\n","    <td> 0.2797765635793422</td> \n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"u2DIdIlWLflM"},"source":["## 6 - Fase: Backward propagation (2pt)\n"]},{"cell_type":"markdown","source":["Com funções auxiliares, a fase back propagation é usada para calcular o gradiente da função loss em relação aos parâmetros. \n","\n","**Lembrete**: \n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=19lwOgDLuWoMzXjHafCPkj3dzZcSyeh0r)\n","\n","<caption><center> **Figura 3** : <br> *Os blocos roxos representam a fase forward propagation, e os vermelhos representam a fase backward propagation.*  </center></caption>\n","\n","\n","\n","Usaremos duas funções, igualmente feito na fase forward:\n","- LINEAR\n","- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID"],"metadata":{"id":"psreXfdrPXu2"}},{"cell_type":"markdown","metadata":{"id":"THlfUk5eLflM"},"source":["### 6.1 - Linear backward\n","\n","Para a camada $l$, a parte linear é: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (seguida por uma ativação).\n","\n","Suponha que $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$ já foi calculado. \n","\n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=19hp00jo-CAsdBswuxFfLsN0JHbsXl5-1)\n","\n","<caption><center> **Figura 4** </center></caption>\n","\n","As saídas $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ são calculadas usando $dZ^{[l]}$:\n","$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n","$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n","$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"]},{"cell_type":"markdown","metadata":{"id":"77ryBJfZLflN"},"source":["**Exercício**: Use as 3 fórmulas acima para implementar linear_backward( )."]},{"cell_type":"code","metadata":{"id":"Kbnstv6ZLflN","executionInfo":{"status":"ok","timestamp":1650409608260,"user_tz":180,"elapsed":35,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# linear_backward\n","\n","def linear_backward(dZ, cache):\n","    \"\"\"\n","    Implementa a parte linear da fase backward propagation em uma camada l\n","\n","    Entradas:\n","    dZ -- gradiente do custo em relação a saída linear da camada l\n","    cache -- tupla (A_prev, W, b) vindo da forward propagation da camada l\n","\n","    Saídas:\n","    dA_prev -- gradiente do custo em relação a ativação da camada l-1,\n","    dW -- gradiente do custo em relação a W  da camada l,\n","    db -- gradiente do custo em relação a b,\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    #print(A_prev.shape[1])\n","    #print(dZ.shape[1])\n","\n","    ### Início do código ### \n","    dW = 1/m * dZ.dot(A_prev.T)\n","    db = 1/m * np.sum(dZ, axis = 1).reshape(dZ.shape[0], 1)\n","    dA_prev = W.T.dot(dZ)\n","    ### Fim do código ###\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db"],"execution_count":152,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-wWOgAJLflQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409608261,"user_tz":180,"elapsed":35,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"ee806aa1-2be0-4066-e97d-e924c9744616"},"source":["# Teste\n","dZ, linear_cache = linear_backward_test_case()\n","\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":153,"outputs":[{"output_type":"stream","name":"stdout","text":["dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n"," [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n"," [-0.4319552  -1.30987417  1.72354705  0.05070578]\n"," [-0.38981415  0.60811244 -1.25938424  1.47191593]\n"," [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n","dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n"," [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n"," [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n","db = [[-0.14713786]\n"," [-0.11313155]\n"," [-0.13209101]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"lOHi3L7mLflT"},"source":["**Valores Esperados**:\n","    \n","```\n","dA_prev = \n"," [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n"," [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n"," [-0.4319552  -1.30987417  1.72354705  0.05070578]\n"," [-0.38981415  0.60811244 -1.25938424  1.47191593]\n"," [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n","dW = \n"," [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n"," [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n"," [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n","db = \n"," [[-0.14713786]\n"," [-0.11313155]\n"," [-0.13209101]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"MWYLQFi7LflT"},"source":["### 6.2 - Linear-Ativação backward\n","\n","A etapa backward para a ativação **`linear_ativacao_backward`**. \n","\n","Use as funções:\n","- **`sigmoid_backward`**: backward propagation para SIGMOID:\n","\n","```python\n","dZ = sigmoid_backward(dA, ativacao_cache)\n","```\n","\n","- **`relu_backward`**: backward propagation para RELU:\n","\n","```python\n","dZ = relu_backward(dA, ativacao_cache)\n","```\n","\n","Se $g(.)$ é a função de ativação, \n","`sigmoid_backward` e `relu_backward` calcula $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n","\n"]},{"cell_type":"code","metadata":{"id":"rlZnbuWnLflU","executionInfo":{"status":"ok","timestamp":1650409608262,"user_tz":180,"elapsed":31,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# linear_ativacao_backward\n","\n","def linear_ativacao_backward(dA, cache, ativacao):\n","    \"\"\"\n","    Implementa a backward propagation para ativação.\n","    \n","    Entradas:\n","    dA -- gradiente da pos-ativacao gradient para camada l \n","    cache -- tupla de valores (linear_cache, ativacao_cache)\n","    ativacao -- \"sigmoid\" or \"relu\"\n","    \n","    Saídas:\n","    dA_prev -- gradiente do custo em relação a ativação da camada l-1,\n","    dW -- gradiente do custo em relação a W  da camada l,\n","    db -- gradiente do custo em relação a b,\n","    \"\"\"\n","    linear_cache, ativacao_cache = cache\n","    \n","    if ativacao == \"relu\":\n","        ### Início do código ### \n","        dZ = relu_backward(dA, ativacao_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        ### Fim do código ###\n","        \n","    elif ativacao == \"sigmoid\":\n","        ### Início do código ### \n","        dZ = sigmoid_backward(dA, ativacao_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        ### Fim do código ###\n","    \n","    return dA_prev, dW, db"],"execution_count":154,"outputs":[]},{"cell_type":"code","metadata":{"id":"gG5XV0YHLflZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409609251,"user_tz":180,"elapsed":1019,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"22b5f8b4-7626-47ce-f8c9-1749021ca44a"},"source":["# Teste\n","\n","dAL, linear_ativacao_cache = linear_activation_backward_test_case()\n","dA_prev, dW, db = linear_ativacao_backward(dAL, linear_ativacao_cache, ativacao = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_ativacao_backward(dAL, linear_ativacao_cache, ativacao = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":155,"outputs":[{"output_type":"stream","name":"stdout","text":["sigmoid:\n","dA_prev = [[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]]\n","dW = [[ 0.10266786  0.09778551 -0.01968084]]\n","db = [[-0.05729622]]\n","\n","relu:\n","dA_prev = [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]]\n","dW = [[ 0.44513824  0.37371418 -0.10478989]]\n","db = [[-0.20837892]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"PJWts-SALflc"},"source":["**Valores esperados com:**\n","\n","<table style=\"width:100%\">\n","  <tr>\n","    <td > dA_prev </td> \n","           <td >[[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]] </td> \n","\n","  </tr> \n","  \n","   <tr>\n","    <td > dW </td> \n","           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n","  </tr> \n","  \n","   <tr>\n","    <td > db </td> \n","           <td > [[-0.05729622]] </td> \n","  </tr> \n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hUZv2-aPLfld"},"source":["**Valores esperados com relu:**\n","\n","<table style=\"width:100%\">\n","  <tr>\n","    <td > dA_prev </td> \n","           <td > [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]] </td> \n","\n","  </tr> \n","  \n","   <tr>\n","    <td > dW </td> \n","           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n","  </tr> \n","  \n","   <tr>\n","    <td > db </td> \n","           <td > [[-0.20837892]] </td> \n","  </tr> \n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d8OsrFC1Lfld"},"source":["### 6.3 - L-Modelo Backward \n","\n","A Figura mostra a fase backward. \n","\n","\n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=19dmT5_6jwKXm92AjDB0F-Z0VxI-RoQYH)\n","<caption><center>  **Figura 5** : Fase Backward  </center></caption>\n","\n","**Inicializando a fase backpropagation**:\n","A saída da rede é, \n","$A^{[L]} = \\sigma(Z^{[L]})$. Então temos que calcualar `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$:\n","\n","`dAL`  $=-\\frac{Y}{AL} + \\frac{1-Y}{1-Al}$ \n","\n","\n","O gradiente `dAL` para continuar propagando. Como visto na Figura 5, `dAL` vai alimentar a linear_ativacao_backward com ativação SIGMOID (que utilizará os valores armazenados em cache armazenados pela função L_modelo_forward). Depois disso, você terá que usar um loop `for` para percorrer todas as outras camadas usando linear_ativacao_backward com ativação RELU. Você deve armazenar cada dA, dW e db no dicionário grads.\n"]},{"cell_type":"code","metadata":{"id":"QVgJXRPHLfle","executionInfo":{"status":"ok","timestamp":1650409609252,"user_tz":180,"elapsed":27,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# L_modelo_backward\n","\n","def L_modelo_backward(AL, Y, caches):\n","    \"\"\"\n","    Implementa a backward propagation para [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n","    \n","    \n","    Entradas:\n","    AL -- Probabiliade de predição da rede, saída da fase forward propagation (L_modelo_forward())\n","    Y -- Vetor de rótulos dos exemplos de treinamento  ( 0 se não tem gato, 1 tem gato )\n","    caches -- lista de caches contendo:\n","                todos cache da linear_ativacao_forward() com \"relu\" ( caches[l], l = 0...L-2)\n","                o cache da linear_ativacao_forward() com \"sigmoid\" (caches[L-1])\n","    \n","    Saídas:\n","    grads -- Um dicionário com os gradientes\n","              \n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # número de camadas\n","    m = AL.shape[1] # número de exemplos\n","    \n","    Y = Y.reshape(AL.shape) # Y deve ter o mesmo formato que AL\n","    \n","    # Inicilizando a fase backpropagation\n","    ### Início do código ### \n","    dAL = - Y/AL + (1 - Y)/(1 - AL) # gradiente do custo em relação a AL\n","    ### Fim do código ###\n","    \n","    # gradiente da l-ésima camada (SIGMOID -> LINEAR). \n","    # Entrada: \"dAL, corrente_cache\". Saida: \"d(AL-1), dWL, dbL\"\n","    ### Início do código ### \n","    current_cache = caches[L-1]\n","    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_ativacao_backward(dAL, current_cache, \"sigmoid\")\n","    ### Fim do código ###\n","    \n","    # Gradientes das camadas anterios: (RELU -> LINEAR)\n","    # Entradas: \"dA(l+1), corrente_cache\". \n","    # Saídas: \"dA(l), dW(l+1), db(l+1)\" \n","    ### Início do código ### \n","    # Loop de l=L-2 até l=0\n","    for l in reversed(range(L-1)):\n","      current_cache = caches[l]\n","      dA_prev_temp, dW_temp, db_temp = linear_ativacao_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\") # dice: ver linear_activation_backward()\n","      grads[\"dA\" + str(l)] = dA_prev_temp\n","      grads[\"dW\" + str(l + 1)] = dW_temp\n","      grads[\"db\" + str(l + 1)] = db_temp\n","    ### Fim do código ###\n","\n","    return grads"],"execution_count":156,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjlRs9F0Lflh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409609253,"user_tz":180,"elapsed":27,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"6974d637-b644-468b-b54e-0fe225553d9d"},"source":["AL, Y_teste, caches = L_model_backward_test_case()\n","\n","grads = L_modelo_backward(AL, Y_teste, caches)\n","print_grads(grads)"],"execution_count":157,"outputs":[{"output_type":"stream","name":"stdout","text":["dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n"," [0.         0.         0.         0.        ]\n"," [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n","db1 = [[-0.22007063]\n"," [ 0.        ]\n"," [-0.02835349]]\n","dA1 = [[ 0.12913162 -0.44014127]\n"," [-0.14175655  0.48317296]\n"," [ 0.01663708 -0.05670698]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"eA-YLjT7Lflj"},"source":["**Valores esperados**\n","\n","<table style=\"width:60%\">\n","  \n","  <tr>\n","    <td > dW1 </td> \n","           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n"," [ 0.          0.          0.          0.        ]\n"," [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n","  </tr> \n","  \n","   <tr>\n","    <td > db1 </td> \n","           <td > [[-0.22007063]\n"," [ 0.        ]\n"," [-0.02835349]] </td> \n","  </tr> \n","  \n","  <tr>\n","  <td > dA1 </td> \n","           <td > [[ 0.12913162 -0.44014127]\n"," [-0.14175655  0.48317296]\n"," [ 0.01663708 -0.05670698]] </td> \n","\n","  </tr> \n","</table>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qvZW8iNXLflk"},"source":["### 6.4 - Atualização dos parâmetros\n","\n","Usando gradiente descendente: \n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n","\n","onde $\\alpha$ é a taxa de aprendizagem."]},{"cell_type":"markdown","metadata":{"id":"LS1_5sdXLflk"},"source":["**Instruções**:\n","Atualização dos parâmetros usando gradiente descendente: $W^{[l]}$ and $b^{[l]}$ para $l = 1, 2, ..., L$. \n"]},{"cell_type":"code","metadata":{"id":"IwPZN1oQLfll","executionInfo":{"status":"ok","timestamp":1650409609253,"user_tz":180,"elapsed":22,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# atualize_parametros\n","\n","def atualize_parametros(parametros, grads, learning_rate):\n","    \"\"\"\n","    Atualização dos parâmetros usando gradiente descendente:\n","    \n","    Entradas:\n","    parametros -- python dicionario contendo os parametros \n","    grads -- python dicionario contendo os gradientes, saída L_modelo_backward\n","    \n","    Saídas:\n","    parametros -- python dicionario contendo os parametros \n","                \n","    \"\"\"\n","    \n","    L = int(len(parametros)/2)  # número de camadas da rede\n","\n","    # Atualiza os parametros.\n","    ### Início do código ###\n","    for l in range(L):\n","        parametros[\"W\" + str(l+1)] -= learning_rate* grads[\"dW\" + str(l+1)]\n","        parametros[\"b\" + str(l+1)] -= learning_rate* grads[\"db\"+ str(l+1)]\n","    ### Fim do código ###\n","    return parametros"],"execution_count":158,"outputs":[]},{"cell_type":"code","metadata":{"id":"wzlnOk-6Lflo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650409609254,"user_tz":180,"elapsed":22,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}},"outputId":"03766d17-d1b1-49f7-c217-e3702d5081f3"},"source":["parametros, grads = update_parameters_test_case()\n","parametros = atualize_parametros(parametros, grads, 0.1)\n","\n","print (\"W1 = \"+ str(parametros[\"W1\"]))\n","print (\"b1 = \"+ str(parametros[\"b1\"]))\n","print (\"W2 = \"+ str(parametros[\"W2\"]))\n","print (\"b2 = \"+ str(parametros[\"b2\"]))"],"execution_count":159,"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n","b1 = [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]]\n","W2 = [[-0.55569196  0.0354055   1.32964895]]\n","b2 = [[-0.84610769]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"yYpdW4jtLflr"},"source":["**Valores esperados**:\n","\n","<table style=\"width:100%\"> \n","    <tr>\n","    <td > W1 </td> \n","           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n","  </tr> \n","  \n","   <tr>\n","    <td > b1 </td> \n","           <td > [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]] </td> \n","  </tr> \n","  <tr>\n","    <td > W2 </td> \n","           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n","  </tr> \n","  \n","   <tr>\n","    <td > b2 </td> \n","           <td > [[-0.84610769]] </td> \n","  </tr> \n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"zNAdvsTALfls"},"source":["## 7 - Construa o modelo (2pt)\n"]},{"cell_type":"markdown","source":["Implemente o modelo usando as funções anteriores para treinar os parâmetros da rede no conjunto de dados."],"metadata":{"id":"LSxQEG3BPl_5"}},{"cell_type":"code","metadata":{"id":"QEf7wROhLflu","executionInfo":{"status":"ok","timestamp":1650409609255,"user_tz":180,"elapsed":19,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# L_layer_modelo\n","\n","def L_layer_modelo(X, Y, camada_dims, learning_rate = 0.0075, num_iter = 3000, print_custo=False):\n","    \"\"\"\n","    Implementa a uma rede neural com L-camadas: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n","    \n","    Entradas:\n","    X -- conjunto de treinamento representado por uma matriz numpy da forma (num_px * num_px * 3, numero de exemplos)\n","    Y -- rótulos de treinamento representados por uma matriz numpy (vetor) da forma (1, numero de exemplos)\n","    camadas_dims -- lista contendo a dimensão dos dados de entrada e tamanho de cada camada da rede, (numero de camadas + 1).\n","    learning_rate -- lhiperparâmetro que representa a taxa de aprendizado usada na regra de atualização do gradiente descendete\n","    num_iter -- hiperparâmetro que representa o número de iterações para otimizar os parâmetros\n","    print_custo -- imprime o custo a cada 100 iterações\n","    \n","    Saida:\n","    parametros -- parametros aprendidos do modelo.\n","    \"\"\"\n","\n","    np.random.seed(1)\n","    custos = []                         # guarda o custo\n","    \n","    # Inicialização dos parametros\n","    ### Início do código ###\n","    parameters = inicialize_parametros(camada_dims) # dica : ver sua função de inicializacao\n","    ### Fim do código ###\n","    \n","    # Gradiente descendente. Dica : use as funções que você escreveu acima\n","    for i in range(0, num_iter):\n","\n","        # Fase Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n","        ### Início do código ###\n","        AL, caches = L_modelo_forward(X, parameters)\n","        ### Fim do código ###\n","        \n","        # Calculo do Custo.\n","        ### Início do código ###\n","        cost = custo(AL, Y)\n","        ### Fim do código ###\n","    \n","        # Fase Backward propagation.\n","        ### Início do código ###\n","        grads = L_modelo_backward(AL, Y, caches)\n","        ### Fim do código ###\n"," \n","        # Atualização dos parametros.\n","        ### Início do código ###\n","        parameters = atualize_parametros(parameters, grads, learning_rate)\n","        ### Fim do código ###\n","                \n","        # Imprime o custo cada 100 iterações\n","        if print_custo and i % 100 == 0:\n","            print (\"Custo depois da iteração %i: %f\" %(i, cost))\n","        if print_custo and i % 100 == 0:\n","            custos.append(cost)\n","            \n","    # plot the cost\n","    plt.plot(np.squeeze(custos))\n","    plt.ylabel('custo')\n","    plt.xlabel('iterações (por centenas)')\n","    plt.title(\"Taxa de aprendizagem =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"],"execution_count":160,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwJRf0vlLflx"},"source":["## 8- Pronto! (1pt)\n"]},{"cell_type":"markdown","source":["\n","### Pre-processamento dos dados\n","\n","Vamos construir o modelo para treinar um classificador de imagens (o mesmo da regressão logística)"],"metadata":{"id":"ANAc11_zPo_-"}},{"cell_type":"code","metadata":{"id":"_YUqWglPXDq8","executionInfo":{"status":"ok","timestamp":1650409609261,"user_tz":180,"elapsed":23,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# Lendo os dados (gato/não-gato)\n","def load_dataset():\n","\n","  train_dataset = h5py.File('/content/drive/MyDrive/disciplinasDoutorado/PCC177-2022-1(Redes)/lab3/train_catvnoncat.h5', \"r\")\n","  train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","  train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","  test_dataset = h5py.File('/content/drive/MyDrive/disciplinasDoutorado/PCC177-2022-1(Redes)/lab3/test_catvnoncat.h5', \"r\")\n","  test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","  test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","  classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","  train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","  test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","  \n","  return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"],"execution_count":161,"outputs":[]},{"cell_type":"code","metadata":{"id":"-d2fWOiJLfly","executionInfo":{"status":"ok","timestamp":1650409609262,"user_tz":180,"elapsed":23,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["# Lendo os dados (gato/não-gato)\n","treino_x_orig, treino_y, teste_x_orig, teste_y, classes = load_dataset()"],"execution_count":162,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNsogn4bLfl1"},"source":["Pre-processamento necessário.\n","\n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=1zCnEB2rwc4lXU_7RTS4TXhqCwsJubg7H)\n","\n","<caption><center> <u>Figura 6</u>: Vetorização de uma imagem. <br> </center></caption>"]},{"cell_type":"code","metadata":{"id":"RcIv5zX9Lfl1","executionInfo":{"status":"ok","timestamp":1650409609263,"user_tz":180,"elapsed":23,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["\n","m_treino = len(treino_x_orig)\n","m_teste = len(teste_x_orig)\n","num_px = teste_x_orig[1].shape[1]\n","\n","#  Vetorizando as imagens de treinamento e teste \n","\n","### Início do código ###\n","treino_x_vet = treino_x_orig.reshape(m_treino, num_px*num_px*3)  # dica : utilize reshape para mudar o formato dos dados\n","teste_x_vet = teste_x_orig.reshape(m_teste, num_px*num_px*3)   # dica : utilize reshape para mudar o formato dos dados\n","treino_x = treino_x_vet.T\n","teste_x = teste_x_vet.T\n","### Fim do código ###\n","\n","### Início do código ###\n","# Normalize os dados para ter valores de recurso entre 0 e 1.\n","treino_x = treino_x/255\n","teste_x = teste_x/255\n","### Fim do código ###\n"],"execution_count":163,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DOzgbH4-Lfl3"},"source":["\n","### Testando com rede neural com 2 camadas\n","\n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=19mux_FFpeZkj5YiV51bNE2CK3i2nBSau)\n","<caption><center> <u>Figura 7</u>: Rede neural com 2 camadas. <br> Resumo do modelo: ***ENTRADA -> LINEAR -> RELU -> LINEAR -> SIGMOID -> SAIDA***. </center></caption>\n","\n","<!--\n","<u>Detailed Architecture of figure 2</u>:\n","- The input is a (64,64,3) image which is flattened to a vector of size $(12288,1)$. \n","- The corresponding vector: $[x_0,x_1,...,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n","- You then add a bias term and take its relu to get the following vector: $[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T$.\n","- You then repeat the same process.\n","- You multiply the resulting vector by $W^{[2]}$ and add your intercept (bias). \n","- Finally, you take the sigmoid of the result. If it is greater than 0.5, you classify it to be a cat.\n","!-->"]},{"cell_type":"code","metadata":{"id":"LKd_8OcsLfl4","executionInfo":{"status":"ok","timestamp":1650409609264,"user_tz":180,"elapsed":23,"user":{"displayName":"DANIELA COSTA TERRA","userId":"00909240082289369654"}}},"source":["### Executar uma rede de 2 camada ###\n","camadas_dims = [12288, 7, 1] \n"," "],"execution_count":164,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1UpnIDtLfl7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6adb8ebb-0972-4460-bc81-b258d5d7e329"},"source":["## Treine a rede\n","parametros = L_layer_modelo(treino_x, treino_y, camadas_dims, num_iter = 2500, print_custo=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Custo depois da iteração 0: 0.692380\n","Custo depois da iteração 100: 0.646159\n","Custo depois da iteração 200: 0.631775\n","Custo depois da iteração 300: 0.600091\n","Custo depois da iteração 400: 0.559427\n","Custo depois da iteração 500: 0.512988\n","Custo depois da iteração 600: 0.454815\n","Custo depois da iteração 700: 0.399388\n","Custo depois da iteração 800: 0.420515\n","Custo depois da iteração 900: 0.369184\n","Custo depois da iteração 1000: 0.362393\n","Custo depois da iteração 1100: 0.341366\n","Custo depois da iteração 1200: 0.333344\n"]}]},{"cell_type":"markdown","metadata":{"id":"yL39tGwTLfl9"},"source":["**Use os parâmetros treinados** para classificar as imagens de treinamento e teste e verificar a acurácia. "]},{"cell_type":"code","metadata":{"id":"MEehDg8DLfl_"},"source":["## Predição da rede\n","# dica : re-utilize e modifique a FUNCAO de predição do Lab1B\n","# GRADED FUNCTION: predição\n","\n","def predicao(X, Y, parametros):\n","    '''\n","    Prediz se o rótulo é 0 ou 1 usando os parâmetros de aprendizagem (w,b) da regressão logística\n","    \n","    Entrada:\n","    X -- dados de treinamentos de tamanho (num_px * num_px * 3, número de exemplos)\n","    parametros -- dicionários de parâmetros da rede treinada\n","    Saída:\n","        Y_pred -- valores da pós-ativação da última camada em que (ativação_AL) > 0.5 é 1, e ativação_AL <=0.5 é 0)\n","    '''\n","    \n","    #ToDo : implemente a função\n","    AL, caches = L_modelo_forward(X, parametros)\n","\n","    # Imprime o custo da execução:\n","    print(\"Custo da execução:{} %\".format(custo(AL, Y)))\n","     \n","    m = X.shape[1]       # número de exemplos. Dica: acesso o shape de X e veja qual valor adequado \n","    \n","    # Converta as proobabilidades AL[0,i] para predição p[0,i]\n","    Y_pred = np.zeros((AL.shape))\n","    positive_pred_index = AL > 0.5\n","    Y_pred[positive_pred_index] =  1    \n","    ### Fim do código ###       \n","   \n","    assert(Y_pred.shape == (1, m))\n","    return Y_pred\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (Daniela) Análise dos resultados do modelo:\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","# Predições do modelo:\n","Y_pred_treino = predicao(treino_x, treino_y, parametros)\n","Y_pred_teste = predicao(teste_x, teste_y, parametros)\n","\n","# Imprime erros do treino/teste \n","print(\"treino acurácia:{} %\".format(100 - np.mean(np.abs(Y_pred_treino - treino_y)) * 100))\n","print(\"teste acurácia: {} %\".format(100 - np.mean(np.abs(Y_pred_teste - teste_y)) * 100))\n","\n","# Exibe matrizes de confusão para dados de treino e teste:\n","cm = confusion_matrix(treino_y[0][:], Y_pred_treino[0][:], labels=[0, 1])\n","disp_treino = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"non cat\", \"cat\"])\n"," \n","cm = confusion_matrix(teste_y[0][:], Y_pred_teste[0][:], labels=[0, 1])\n","disp_teste = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"non cat\", \"cat\"])\n","\n","plt.figure(figsize=(4, 2)) \n","disp_treino.plot()\n","plt.title(\"Treinamento: Cat vs Non-cat)\")\n","disp_teste.plot()\n","plt.title(\"Teste: Cat vs Non-cat)\")\n","plt.show()\n","plt.tight_layout()  "],"metadata":{"id":"OjoUqH2yWp3R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otlPGXyzBRuP"},"source":["Resultado esperado:\n","\n","Acurácia treino = 100%\n","\n","Acurácia teste = 72%\n","\n","Por que você obteve 100% no treino e apenas 72% no teste?\n","\n","Resposta: novamente houve sobreajuste dos parâmetros da rede para o conjunto dos dados de treino. Além disso, os dados de treino estão desbalanceados para o número de classes(72 cat e 137 non-cat)."]},{"cell_type":"markdown","metadata":{"id":"4FN1QQO4LfmB"},"source":["### Testando com uma rede com 4 camadas\n","\n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=19h9LuWkWLVMYgAAoQKTfjJ-Er-tlw8En)\n","<caption><center> <u>Figura 8</u>: Rede neural com L camadas. <br> Resumo do modelo: ***ENTRADA -> LINEAR -> RELU -> LINEAR -> SIGMOID -> SAIDA***. </center></caption>"]},{"cell_type":"code","metadata":{"id":"AyZpNGpcLfmC"},"source":["### Executar uma rede de 4 camada ###\n","camadas_dims = [12288, 20, 7, 5, 1]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5aA5aR1LfmF"},"source":["## Treine a rede\n","parametros = L_layer_modelo(treino_x, treino_y, camadas_dims, num_iter = 2500, print_custo=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lVRI71VlLfmI"},"source":["**Use os parâmetros treinados** para classificar as imagens de treinamento e teste e verificar a acurácia. "]},{"cell_type":"code","metadata":{"id":"bf8WzSsULfmI"},"source":["# dica : re-utilize e modifique a FUNCAO de predição do Lab1B\n"," \n","# Predições do modelo:\n","Y_pred_treino = predicao(treino_x, treino_y, parametros)\n","Y_pred_teste = predicao(teste_x, teste_y, parametros)\n","\n","# Imprime erros do treino/teste \n","print(\"treino acurácia:{} %\".format(100 - np.mean(np.abs(Y_pred_treino - treino_y)) * 100))\n","print(\"teste acurácia: {} %\".format(100 - np.mean(np.abs(Y_pred_teste - teste_y)) * 100))\n","\n","# Exibe matrizes de confusão para dados de treino e teste:\n","cm = confusion_matrix(treino_y[0][:], Y_pred_treino[0][:], labels=[0, 1])\n","disp_treino = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"non cat\", \"cat\"])\n"," \n","cm = confusion_matrix(teste_y[0][:], Y_pred_teste[0][:], labels=[0, 1])\n","disp_teste = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"non cat\", \"cat\"])\n","\n","plt.figure(figsize=(4, 2)) \n","disp_treino.plot()\n","plt.title(\"Treinamento: Cat vs Non-cat)\")\n","disp_teste.plot()\n","plt.title(\"Teste: Cat vs Non-cat)\")\n","plt.show()\n","plt.tight_layout()  \n","\n","# dica2: compute a matirz de confusão"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Qg857Y9-Z7W"},"source":["Resultado esperado:\n","\n","Acurácia no treino: 0.6555023923444976 \n","\n","Acurácia no teste: 0.34\n","\n","Este resultado foi melhor ou pior do que com duas camadas? Tente explicar os motivos.\n","\n","Resposta: esse resultado foi pior pois o modelo ficou propenso a classificação de toda exemplar como um não gato. A curva e os custos exibidos demonstram que a rede não foi treinada com um número de iterações suficiente para convergência (após 2500, custo = 0.64). Para uma rede mais complexas seria necessário mais iterações para treinar o modelo."]},{"cell_type":"markdown","metadata":{"id":"luBAEzfiS7x8"},"source":["# **Parte 2** - Classificação de múltiplas classes e uso de frameworks\n","\n"]},{"cell_type":"markdown","source":["No exemplo anterior, usamos uma arquitetura para classificação binária. Para classificaçõ de múltiplas classes, tem-se um neurônio de saída para cada classe (como ilustrado no exemplo da Figura 9) e deve-se usar a operação Softmax antes de se calcular o custo (entropia cruzada ou cross-entropy como no exemplo anterior). Consute o capítulo [3.6 do livro](http://d2l.ai/chapter_linear-networks/softmax-regression-scratch.html) para entender melhor.  No caso de se usar softmax, deve-se usar a função *one_hot* para transformar a saída em logits. Veja a função *one_hot* fornecida. Ela transforma um escalar em um *hot encoder*, de acordo com o número de classes.\n","\n","![Arq,widht=10](https://drive.google.com/uc?export=view&id=1WV_4AT49bYcqsp6PB0FoO4p-gASo0bjL)<caption><center> <u>Figura 9</u>: Rede neural dois neurônios de saída. <br> </center></caption>\n","\n","\n"],"metadata":{"id":"uOdSWNaHRnKn"}},{"cell_type":"code","metadata":{"id":"KcJ7w8v1WBUx"},"source":["# nclasses : numero de classes do prolema, y : um escalar ou vetor de escalares\n","def one_hot(n_classes, y):\n","    return np.eye(n_classes)[y]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcMQfNh4WEKO"},"source":["# ToDo : execute o exemplo e veja o resultado para 4 escalares no vetor de variáveis dependentes\n","one_hot(n_classes=10, y=[0, 4, 9, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"54dwXMXHWTCt"},"source":["### Função softmax \n","\n","A função softmax transforma a saĩda em uma distribuição de probabilidades. Assim, a soma de todas as saídas dos neurônio da última camada sempre vai ser igual a 1:\n","\n","$$\n","softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n","\\cdot\n","\\begin{bmatrix}\n","  e^{x_1}\\\\\\\\\n","  e^{x_2}\\\\\\\\\n","  \\vdots\\\\\\\\\n","  e^{x_n}\n","\\end{bmatrix}\n","$$\n","\n","o gradiente para o custo usando-se a função softmax é trivial de se calcular:\n","\n","$$dw = softmax(\\mathbf{y_{pred}}) - y$$"]},{"cell_type":"code","metadata":{"id":"6XKZaXQUWXTE"},"source":["def softmax(X):\n","    exp = np.exp(X)\n","    return exp / np.sum(exp, axis=-1, keepdims=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovN7neApWfel"},"source":["# ToDo : teste sua função softmax com a instância do exemplo abaixo \n","print(softmax([10, 2, -3]))\n","\n","# As saídas individuais devem ser entre 0 e 1 de forma que a soma seja 1. lembre-se, com softmax, temos probabilidades.\n","print(np.sum(softmax([10, 2, -3])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQJhGae7Wqpr"},"source":["Perceba que nosso código também funciona se você passar um lote (batch) de amostras"]},{"cell_type":"code","metadata":{"id":"_AVEyZmQWpJx"},"source":["# Veja a saída abaixo\n","X = np.array([[10, 2, -3],\n","              [-1, 5, -20]])\n","print(softmax(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_plFTAbuW64u"},"source":["Em seguida, deve-se computar o erro entre um vetor predito Y_pred e o vetor de rótulos Y_true. para tal, deve-se usar cross entropy loss, ou verossimilhança negativa (negative log likelihood). A função cross_entrppy() implementa a verossimilhança negativa."]},{"cell_type":"code","metadata":{"id":"bzgQbzRlW5WU"},"source":["def cross_entropy(Y_true, Y_pred):\n","    EPSILON = 1e-8\n","  \n","\n","    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n","    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n","    \n","    return -np.mean(loglikelihoods)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZBY_8TGXAB0"},"source":["verifique o erro de uma predição bem ruim"]},{"cell_type":"code","metadata":{"id":"H78KTS8MXB8L"},"source":["print(cross_entropy([1, 0, 0], softmax([0.12, 4, 10])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hSEH_4N5XE6r"},"source":["verifique o erro de uma boa predição"]},{"cell_type":"code","metadata":{"id":"XXArje6XXGi3"},"source":["print(cross_entropy([1, 0, 0], [0.98, 0.01, .01]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0bf2ChgHXJjT"},"source":["A função cross_entropy() também deve funcionar para um lote de dados"]},{"cell_type":"code","metadata":{"id":"Fx1hEVroXMXx"},"source":["# Verifique a cross-entropy das três amostas seguintes:\n","\n","Y_true = np.array([[0, 1, 0],\n","                   [1, 0, 0],\n","                   [0, 0, 1]])\n","\n","Y_pred = np.array([[0,   1,    0],\n","                   [.99, 0.01, 0],\n","                   [0,   0,    1]])\n","\n","# repare que as amostras são praticamente predições perfeitas\n","print(cross_entropy(Y_true, Y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D0qdtdxIZtUb"},"source":["## Pré-processamento dos dados\n","\n","Vamos usar a biblioteca scikit learn para nos auxiliar na execução da prática. \n","Veja a documentação em https://scikit-learn.org/stable/index.html"]},{"cell_type":"markdown","metadata":{"id":"IDJpmPmhZx3_"},"source":["Considere a base de dados abaixo. Ela é referente a um atividade em um site de vendas qualquer. O objetivo com esta base é tentar predizer quais clientes futuros terão probabildiade de comprar algum produto, com base em algumas características, como cidade em que mora, idade e salário.\n","\n","*Carregando os dados*"]},{"cell_type":"code","metadata":{"id":"ShTbKDBbZ0ml"},"source":["# Importe as bibliotecas NumPy, Pandas e Matplotlib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# carregue os dados do arquivo e armazenar em um Dataframe\n","dataset = pd.read_csv('/content/drive/MyDrive/disciplinasDoutorado/PCC177-2022-1(Redes)/lab3/Data.csv')\n","\n","# imprima a estrutura dataset\n","print(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0X189fCBZ4Ib"},"source":["Crie dois objetos, uma chamado X, para receber as caracteísticas das instâncias e um chamado y para receber as classes. Observe que as instâncias devem ser organizadas em linha Assm, as características da linha 0 de X devem corresponder a classe da linha 0 de y Podemos chamar as variáveis de X (ou características -usadas para fazer a predição) de variáveis independentes e a variável de y (classe a ser predita) de variável dependente."]},{"cell_type":"code","metadata":{"id":"X3ODN_jkZ7sr"},"source":["X = dataset.iloc[:, :-1].values\n","y = dataset.iloc[:, 3].values\n","\n","# imprima X e y\n","print(X)\n","print(y)\n","\n","# imprima e analise o formato dos objetos\n","print(X.shape)\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAxJX2xkaB23"},"source":["from sklearn.impute import SimpleImputer\n","#imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n","imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","imputer.fit(X[:, 1:3])\n","X[:, 1:3] = imputer.transform(X[:, 1:3])\n","\n","# imprima a nova matriz X\n","print(X)"],"execution_count":null,"outputs":[]}]}